{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "revolutionary-antenna",
   "metadata": {},
   "source": [
    "# Bayes Theorem Report CS361 A3\n",
    "#### UPI: ytia165, ID: 402799865\n",
    "*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-crossing",
   "metadata": {},
   "source": [
    "It should explain and motivate the chosen representation & data preprocessing, then the method extensions, their implementation (including the implementation of the standard Naive Bayes) and the performance (training and validation) results for the standard and extended Naive Bayes method; Please comment how did you perform model evaluation/validation.\n",
    "You can summarize results using tables (or plots), but all results have to be explained descriptively as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-direction",
   "metadata": {},
   "source": [
    "# Code for Classifier Based on Text Inputs\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "embedded-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-visitor",
   "metadata": {},
   "source": [
    "### Reading in CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pharmaceutical-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "train_text = []\n",
    "with open(\"trg.csv\") as input_csv:\n",
    "    reader = csv.reader(input_csv, delimiter=\",\", quotechar='\"')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        classes.append(row[1])\n",
    "        train_text.append(row[2])\n",
    "        \n",
    "# change categorical classes to numeric class\n",
    "unique_classes = sorted(set(classes))\n",
    "class_to_id = {x: unique_classes.index(x) for x in unique_classes}\n",
    "id_to_class = dict([(value, key) for key, value in class_to_id.items()])\n",
    "classes_numeric = np.array([class_to_id[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-reaction",
   "metadata": {},
   "source": [
    "### Bayes Theorem and Validation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acquired-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y, test_split):\n",
    "    '''\n",
    "    Returns training and test set \n",
    "\n",
    "            Parameters (in this instance):\n",
    "                    x (list/numpy.ndarry): A list of explanatory variables\n",
    "                    y (list/numpy.ndarry): A list of response variables \n",
    "                    test_split (int): The proprotion cutoff of test to train sets\n",
    "\n",
    "            Returns:\n",
    "                    x[train], x[test] (numpy.ndarry): explanatory training and test set\n",
    "                    y[train], y[test] (numpy.ndarry): response training and test set\n",
    "    '''\n",
    "    n_test = int(test_split * len(x))\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    perm = np.random.default_rng(seed = 402799865).permutation(len(x))\n",
    "    test, train = perm[:n_test], perm[n_test:]\n",
    "    \n",
    "    return x[train], x[test], y[train], y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "independent-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_prob(y_train):\n",
    "    '''Returns the probability of a class (format: dictionary)'''\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    dicY_counts = dict(zip(unique, counts))\n",
    "    dicY_prob = {}\n",
    "    for key, value in dicY_counts.items():\n",
    "        dicY_prob[key] = value/sum(dicY_counts.values())\n",
    "        \n",
    "    return dicY_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "documented-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary(x_train, y_train, stopwords = []):\n",
    "    '''\n",
    "    Returns all given words and words subsetted by class\n",
    "\n",
    "            Parameters (in this instance):\n",
    "                    x_train (numpy.ndarry): A list of explanatory variables\n",
    "                    y_train (numpy.ndarry): A list of response variables \n",
    "                    stopwords (list): Words to not include in the dictionary\n",
    "\n",
    "            Returns:\n",
    "                    dictWordsFull (dictionary): All words \n",
    "                    listdict (dictionaries in list): All words subsetted by class\n",
    "    '''\n",
    "    dictWordsFull = {}\n",
    "    listdict = [{} for _ in range(len(unique_classes))]\n",
    "    for array in range(0, len(x_train)-1):\n",
    "        for word in x_train[array].split(): \n",
    "            if word not in stopwords:\n",
    "                if word not in dictWordsFull:\n",
    "                    dictWordsFull[word] = 1  \n",
    "                else:\n",
    "                    dictWordsFull[word] += 1\n",
    "\n",
    "                if word not in listdict[y_train[array]]:\n",
    "                    listdict[y_train[array]][word] = 1\n",
    "                else:\n",
    "                    listdict[y_train[array]][word] += 1\n",
    "                    \n",
    "    return dictWordsFull, listdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "royal-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_prob_log(dictWordsFull, listdict):\n",
    "    '''Returns the probability of a word appearing given the class (format: dictionaries in list)'''\n",
    "    listdictprob = [{} for _ in range(len(unique_classes))]\n",
    "    for key in dictWordsFull:\n",
    "        for classi in range(0,len(unique_classes)):\n",
    "            if key not in listdict[classi]:\n",
    "                listdictprob[classi][key] = 1/(sum(listdict[classi].values())+len(dictWordsFull))\n",
    "            else:\n",
    "                listdictprob[classi][key] = (listdict[classi][key]+1)/(sum(listdict[classi].values())+len(dictWordsFull))\n",
    "                \n",
    "    return listdictprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lyric-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_calculate(words, listdictprob, dicY_prob):\n",
    "    '''Returns the most likely class based on Baysean Probability'''\n",
    "    classes = [[prob] for prob in dicY_prob.values()]\n",
    "    for word in words.split():\n",
    "        for classi in range(0,len(unique_classes)):\n",
    "            if word in listdictprob[classi]:\n",
    "                classes[classi].append(listdictprob[classi][word])\n",
    "    \n",
    "    maxSum = []\n",
    "    for lst in classes:\n",
    "        maxSum.append(np.prod(lst))\n",
    "\n",
    "    return maxSum.index(max(maxSum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "colored-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(x_test, listdictprob, dicY_prob, y_test):\n",
    "    '''Returns accuracy based on x and y test sets'''\n",
    "    test_array = []\n",
    "    for array in x_test:\n",
    "        test_class = class_calculate(array, listdictprob, dicY_prob)\n",
    "        test_array.append(test_class)\n",
    "    test_array_class = np.array([id_to_class[c] for c in test_array])\n",
    "    \n",
    "    return sum(1 for x,y in zip(test_array,y_test) if x == y) / len(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "critical-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classi_identifier_test(string, classi, test_split = 0.2):\n",
    "    '''\n",
    "    Returns accuracy of model\n",
    "\n",
    "            Parameters (in this instance):\n",
    "                    string (list/numpy.ndarry): A list of explanatory variables\n",
    "                    classi (list/numpy.ndarry): A list of response variables \n",
    "\n",
    "            Returns:\n",
    "                    accuracy (int): The accuracy of the model\n",
    "    '''\n",
    "    x_train, x_test, y_train, y_test = train_test_split(string, classi, test_split)\n",
    "    dicY_prob = class_prob(y_train)\n",
    "    dictWordsFull, listdict = dictionary(x_train, y_train)\n",
    "    listdictprob = dictionary_prob_log(dictWordsFull, listdict)\n",
    "    accuracy = test_accuracy(x_test, listdictprob, dicY_prob, y_test)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "annoying-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n"
     ]
    }
   ],
   "source": [
    "print(classi_identifier_test(train_text, classes_numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dirty-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classi_identifier_test_kfold(string, classi, fold = 10):\n",
    "    '''Returns accuracy of model based on k-fold cross validation'''\n",
    "    accuracy_lst = []\n",
    "    perm = np.random.default_rng(seed = 402799865).permutation(len(string))\n",
    "    chunks = [perm[x:x+int(len(perm)/fold)] for x in range(0, len(perm), int(len(perm)/fold))]\n",
    "    string, classi = np.array(string), np.array(classi)\n",
    "    \n",
    "    for i in range(0,fold):\n",
    "        training_lst = chunks[:i] + chunks[i+1:]\n",
    "        training_lst_concat = [j for i in training_lst for j in i]\n",
    "        string_train, classi_train = string[training_lst_concat], classi[training_lst_concat]\n",
    "        string_test, classi_test = string[chunks[i]], classi[chunks[i]]\n",
    "        \n",
    "        dicY_prob = class_prob(classi_train)\n",
    "        dictWordsFull, listdict = dictionary(string_train, classi_train)\n",
    "        listdictprob = dictionary_prob_log(dictWordsFull, listdict)\n",
    "        accuracy = test_accuracy(string_test, listdictprob, dicY_prob, classi_test)\n",
    "        accuracy_lst.append(accuracy)\n",
    "        \n",
    "    return accuracy_lst, np.mean(accuracy_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dying-surrey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.2075, 0.185, 0.2175, 0.18, 0.1875, 0.1925, 0.185, 0.17, 0.19, 0.195], 0.191)\n"
     ]
    }
   ],
   "source": [
    "print(classi_identifier_test_kfold(train_text, classes_numeric))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-parking",
   "metadata": {},
   "source": [
    "# Training with Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "criminal-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "with open(\"tst.csv\") as input_csv:\n",
    "    reader = csv.reader(input_csv, delimiter=\",\", quotechar='\"')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        abstracts.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "everyday-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_output(xpreds, listdictprob, dicY_prob):\n",
    "    '''Class Identifier but for full dataset'''\n",
    "    lst = []\n",
    "    for array in xpreds:\n",
    "        class_pred = class_calculate(array, listdictprob, dicY_prob)\n",
    "        lst.append(class_pred)\n",
    "    lst2 = np.array([id_to_class[c] for c in lst])\n",
    "    \n",
    "    return lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "understood-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classi_identifier_full(string, classi, xpreds, to_csv = False, stopw = []):\n",
    "    '''MAIN Class Identifier but for full dataset'''\n",
    "    dicY_prob = class_prob(classi)\n",
    "    dictWordsFull, listdict = dictionary(string, classi, stopwords)\n",
    "    listdictprob = dictionary_prob_log(dictWordsFull, listdict)\n",
    "    output = class_output(xpreds, listdictprob, dicY_prob)\n",
    "    if to_csv == True:\n",
    "        np.savetxt(\"ytia165_CS361_A3_PREDICTIONS2.csv\", output, delimiter =\", \", fmt ='% s')\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "advised-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "organizational-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try = classi_identifier_full(train_text, classes_numeric, abstracts, to_csv = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-michael",
   "metadata": {},
   "source": [
    "### Incorporating stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "delayed-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_try = classi_identifier_full(train_text, classes_numeric, abstracts, to_csv = False, stopw = stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-consultancy",
   "metadata": {},
   "source": [
    "### Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "color-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_prob_complement(dictWordsFull, listdict):\n",
    "    '''Returns the probability of a word appearing given the class (format: dictionaries in list)'''\n",
    "    listdictprob = [{} for _ in range(len(unique_classes))]\n",
    "    for key in dictWordsFull:\n",
    "        for classi in range(0,len(unique_classes)):\n",
    "            listdictprob[classi][key] = 1/((dictWordsFull[key]-listdict[classi][key]+1)/(sum(dictWordsFull.values())-sum(listdict[classi].values())))\n",
    "                \n",
    "    return listdictprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "strange-novel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def class_calculate_mult(words, listdictprob, dicY_prob):\n",
    "    '''Returns the most likely class based on Baysean Probability'''\n",
    "    classes = [[prob] for prob in dicY_prob.values()]\n",
    "    for word in words.split():\n",
    "        for classi in range(0,len(unique_classes)):\n",
    "            if word in listdictprob[classi]:\n",
    "                classes[classi].append(listdictprob[classi][word])\n",
    "    \n",
    "    minSum = []\n",
    "    for lst in classes:\n",
    "        minSum.append(np.prod(lst))\n",
    "\n",
    "    return minSum.index(min(minSum))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
