{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accredited-flight",
   "metadata": {},
   "source": [
    "# Bayes Theorem Classifier Report CS361 A3\n",
    "#### UPI: ytia165, ID: 402799865\n",
    "*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-handbook",
   "metadata": {},
   "source": [
    "The CSV file given contains 3 columns, ID, class, and abstract. The start of my data preprocessing consists of reading the classes and abstracts of each row and converting the unique classes into numerical classes. \n",
    "\n",
    "Next comes my functions which include the implementation of various models of Naive Bayes and the validation functions to test these implementations. These functions are clearly described within the docstring. The first method I used was the standard Naive Bayes model and with a 80/20 train/test split I yielded an accuracy of 0.1975. Since the test set could have been in favour for my model, I used 10-fold cross validation to get an average accuracy for the model. This came out to be \n",
    "\n",
    "| Split         | Accuracy      | \n",
    "| ------------- |---------------|\n",
    "| 1             | 0.2075        | \n",
    "| 2             | 0.185         |\n",
    "| 3             | 0.2175        | \n",
    "| 4             | 0.18          | \n",
    "| 5             | 0.1875        | \n",
    "| 6             | 0.1925        | \n",
    "| 7             | 0.185         | \n",
    "| 8             | 0.17          | \n",
    "| 9             | 0.19          | \n",
    "| 10            | 0.195         | \n",
    "| AVG           | 0.191         | \n",
    "\n",
    "Clearly, the standard implementation was extremely poor as a performed much worse than a 'majority class' model. Looking at several extenstions for Naive Bayes, I decided to implement the log version to smooth down words that appear a lot exponentially. With a quick log change and summing to find the maximum, our 80/20 test yielded an accuracy of 0.94, a vast increase from the basic model. Here is the 10-fold cross validation for a more accuracte accuracy. \n",
    "\n",
    "| Split         | Accuracy      | \n",
    "| ------------- |---------------|\n",
    "| 1             | 0.9475        | \n",
    "| 2             | 0.935         |\n",
    "| 3             | 0.9425        | \n",
    "| 4             | 0.93          | \n",
    "| 5             | 0.9475        | \n",
    "| 6             | 0.92          | \n",
    "| 7             | 0.9525        | \n",
    "| 8             | 0.935         | \n",
    "| 9             | 0.9425        | \n",
    "| 10            | 0.9325        | \n",
    "| AVG           | 0.9385        | \n",
    "\n",
    "An average accuracy of 93.85% was immensely better at classifying classes based on text probability. Looking at our dictionary of words, we have common words appearing many times that have no or little relation with the class. Some of these words include 'The', 'It', and 'And'. With this knowledge, we make our improved model where if we see these words, we will not include them in our probability dictionary. You can find the more comprehensive list in the variable 'stopwords'. Running this model on 80/20 we get the same accuracy of 0.94. However, running our 10-fold cross validation yields different results. Here is the accuracy for stopword removal.\n",
    "\n",
    "| Split         | Accuracy      | \n",
    "| ------------- |---------------|\n",
    "| 1             | 0.95          | \n",
    "| 2             | 0.9525        |\n",
    "| 3             | 0.945         | \n",
    "| 4             | 0.9375        | \n",
    "| 5             | 0.96          | \n",
    "| 6             | 0.935         | \n",
    "| 7             | 0.9525        | \n",
    "| 8             | 0.94          | \n",
    "| 9             | 0.945         | \n",
    "| 10            | 0.9425        | \n",
    "| AVG           | 0.9460        | \n",
    "\n",
    "An average accuracy of 94.60% was a tad better than our logged Naive bayes without stopwords. Here is a summary of the 3 models. \n",
    "\n",
    "| Standard | Logged | Logged + Stopwords |\n",
    "| :---: | :---: | :---: |\n",
    "| 0.191 | 0.9385 | 0.9460 |\n",
    "\n",
    "Finally, looking at the raw data, we have an unbalanced number of classes. Class 'E' is our most common with 2144 appearences and class 'V' is our least common with only 216 appearences out of our 4000 data rows. This is a prime example in where a complement Naive Bayes model might be a better fit for this data. Here, we take the inverse of a normal Naive bayes model. Unfortunately, this was unable to be implemented as a further model since there seems to be a bug somewhere in the code. This is only giving us an accuracy of around 50% which clearly should not be the case. \n",
    "\n",
    "However, with the logged stopword removal model, this was submitted onto Kaggle with a public leaderboard accuracy of 94.6%. The functions were finally cleaned and documented. Here is the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-episode",
   "metadata": {},
   "source": [
    "# Code for Classifier Based on Text Inputs\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "outside-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-stanley",
   "metadata": {},
   "source": [
    "### Reading in CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ordinary-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "train_text = []\n",
    "with open(\"trg.csv\") as input_csv:\n",
    "    reader = csv.reader(input_csv, delimiter=\",\", quotechar='\"')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        classes.append(row[1])\n",
    "        train_text.append(row[2])\n",
    "        \n",
    "# change categorical classes to numeric class\n",
    "unique_classes = sorted(set(classes))\n",
    "class_to_id = {x: unique_classes.index(x) for x in unique_classes}\n",
    "id_to_class = dict([(value, key) for key, value in class_to_id.items()])\n",
    "classes_numeric = np.array([class_to_id[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-joining",
   "metadata": {},
   "source": [
    "### Bayes Theorem and Validation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generous-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y, test_split):\n",
    "    '''\n",
    "    Returns training and test set \n",
    "\n",
    "            Parameters (in this instance):\n",
    "                    x (list/numpy.ndarry): A list of explanatory variables\n",
    "                    y (list/numpy.ndarry): A list of response variables \n",
    "                    test_split (int): The proprotion cutoff of test to train sets\n",
    "\n",
    "            Returns:\n",
    "                    x[train], x[test] (numpy.ndarry): explanatory training and test set\n",
    "                    y[train], y[test] (numpy.ndarry): response training and test set\n",
    "    '''\n",
    "    n_test = int(test_split * len(x))\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    perm = np.random.default_rng(seed = 402799865).permutation(len(x))\n",
    "    test, train = perm[:n_test], perm[n_test:]\n",
    "    \n",
    "    return x[train], x[test], y[train], y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recognized-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_prob(y_train):\n",
    "    '''Returns the probability of a class (format: dictionary)'''\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    dicY_counts = dict(zip(unique, counts))\n",
    "    dicY_prob = {}\n",
    "    for key, value in dicY_counts.items():\n",
    "        dicY_prob[key] = value/sum(dicY_counts.values())\n",
    "        \n",
    "    return dicY_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "living-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary(x_train, y_train, stopwords = []):\n",
    "    '''\n",
    "    Returns all given words and words subsetted by class\n",
    "\n",
    "            Parameters (in this instance):\n",
    "                    x_train (numpy.ndarry): A list of explanatory variables\n",
    "                    y_train (numpy.ndarry): A list of response variables \n",
    "                    stopwords (list): Words to not include in the dictionary\n",
    "\n",
    "            Returns:\n",
    "                    dictWordsFull (dictionary): All words \n",
    "                    listdict (dictionaries in list): All words subsetted by class\n",
    "    '''\n",
    "    dictWordsFull = {}\n",
    "    listdict = [{} for _ in range(len(unique_classes))]\n",
    "    for array in range(0, len(x_train)-1):\n",
    "        for word in x_train[array].split(): \n",
    "            if word not in stopwords:\n",
    "                if word not in dictWordsFull:\n",
    "                    dictWordsFull[word] = 1  \n",
    "                else:\n",
    "                    dictWordsFull[word] += 1\n",
    "\n",
    "                if word not in listdict[y_train[array]]:\n",
    "                    listdict[y_train[array]][word] = 1\n",
    "                else:\n",
    "                    listdict[y_train[array]][word] += 1\n",
    "                    \n",
    "    return dictWordsFull, listdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "macro-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_prob_log(dictWordsFull, listdict):\n",
    "    '''Returns the probability of a word appearing given the class (format: dictionaries in list)'''\n",
    "    listdictprob = [{} for _ in range(len(unique_classes))]\n",
    "    for key in dictWordsFull:\n",
    "        for classi in range(0,len(unique_classes)):\n",
    "            if key not in listdict[classi]:\n",
    "                listdictprob[classi][key] = np.log(1/(sum(listdict[classi].values())+len(dictWordsFull)))\n",
    "            else:\n",
    "                listdictprob[classi][key] = np.log((listdict[classi][key]+1)/(sum(listdict[classi].values())+len(dictWordsFull)))\n",
    "                \n",
    "    return listdictprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incident-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_calculate(words, listdictprob, dicY_prob):\n",
    "    '''Returns the most likely class based on Baysean Probability'''\n",
    "    classes = [[np.log(prob)] for prob in dicY_prob.values()]\n",
    "    for word in words.split():\n",
    "        for classi in range(0,len(unique_classes)):\n",
    "            if word in listdictprob[classi]:\n",
    "                classes[classi].append(listdictprob[classi][word])\n",
    "    \n",
    "    maxSum = []\n",
    "    for lst in classes:\n",
    "        maxSum.append(np.sum(lst))\n",
    "\n",
    "    return maxSum.index(max(maxSum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "present-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(x_test, listdictprob, dicY_prob, y_test):\n",
    "    '''Returns accuracy based on x and y test sets'''\n",
    "    test_array = []\n",
    "    for array in x_test:\n",
    "        test_class = class_calculate(array, listdictprob, dicY_prob)\n",
    "        test_array.append(test_class)\n",
    "    test_array_class = np.array([id_to_class[c] for c in test_array])\n",
    "    \n",
    "    return sum(1 for x,y in zip(test_array,y_test) if x == y) / len(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sitting-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classi_identifier_test(string, classi, test_split = 0.2):\n",
    "    '''\n",
    "    Returns accuracy of model\n",
    "\n",
    "            Parameters (in this instance):\n",
    "                    string (list/numpy.ndarry): A list of explanatory variables\n",
    "                    classi (list/numpy.ndarry): A list of response variables \n",
    "\n",
    "            Returns:\n",
    "                    accuracy (int): The accuracy of the model\n",
    "    '''\n",
    "    x_train, x_test, y_train, y_test = train_test_split(string, classi, test_split)\n",
    "    dicY_prob = class_prob(y_train)\n",
    "    dictWordsFull, listdict = dictionary(x_train, y_train)\n",
    "    listdictprob = dictionary_prob_log(dictWordsFull, listdict)\n",
    "    accuracy = test_accuracy(x_test, listdictprob, dicY_prob, y_test)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deadly-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n"
     ]
    }
   ],
   "source": [
    "print(classi_identifier_test(train_text, classes_numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "local-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classi_identifier_test_kfold(string, classi, fold = 10):\n",
    "    '''Returns accuracy of model based on k-fold cross validation'''\n",
    "    accuracy_lst = []\n",
    "    perm = np.random.default_rng(seed = 402799865).permutation(len(string))\n",
    "    chunks = [perm[x:x+int(len(perm)/fold)] for x in range(0, len(perm), int(len(perm)/fold))]\n",
    "    string, classi = np.array(string), np.array(classi)\n",
    "    \n",
    "    for i in range(0,fold):\n",
    "        training_lst = chunks[:i] + chunks[i+1:]\n",
    "        training_lst_concat = [j for i in training_lst for j in i]\n",
    "        string_train, classi_train = string[training_lst_concat], classi[training_lst_concat]\n",
    "        string_test, classi_test = string[chunks[i]], classi[chunks[i]]\n",
    "        \n",
    "        dicY_prob = class_prob(classi_train)\n",
    "        dictWordsFull, listdict = dictionary(string_train, classi_train)\n",
    "        listdictprob = dictionary_prob_log(dictWordsFull, listdict)\n",
    "        accuracy = test_accuracy(string_test, listdictprob, dicY_prob, classi_test)\n",
    "        accuracy_lst.append(accuracy)\n",
    "        \n",
    "    return accuracy_lst, np.mean(accuracy_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "searching-running",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.9475, 0.935, 0.9425, 0.93, 0.9475, 0.92, 0.9525, 0.935, 0.9425, 0.9325], 0.9385)\n"
     ]
    }
   ],
   "source": [
    "print(classi_identifier_test_kfold(train_text, classes_numeric))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-fiber",
   "metadata": {},
   "source": [
    "# Training with Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "documentary-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "with open(\"tst.csv\") as input_csv:\n",
    "    reader = csv.reader(input_csv, delimiter=\",\", quotechar='\"')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        abstracts.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "informational-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_output(xpreds, listdictprob, dicY_prob):\n",
    "    '''Class Identifier but for full dataset'''\n",
    "    lst = []\n",
    "    for array in xpreds:\n",
    "        class_pred = class_calculate(array, listdictprob, dicY_prob)\n",
    "        lst.append(class_pred)\n",
    "    lst2 = np.array([id_to_class[c] for c in lst])\n",
    "    \n",
    "    return lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "saving-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classi_identifier_full(string, classi, xpreds, to_csv = False, stopw = []):\n",
    "    '''MAIN Class Identifier but for full dataset'''\n",
    "    dicY_prob = class_prob(classi)\n",
    "    dictWordsFull, listdict = dictionary(string, classi, stopwords)\n",
    "    listdictprob = dictionary_prob_log(dictWordsFull, listdict)\n",
    "    output = class_output(xpreds, listdictprob, dicY_prob)\n",
    "    if to_csv == True:\n",
    "        np.savetxt(\"ytia165_CS361_A3_PREDICTIONS2.csv\", output, delimiter =\", \", fmt ='% s')\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "modified-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "resistant-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try = classi_identifier_full(train_text, classes_numeric, abstracts, to_csv = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-theme",
   "metadata": {},
   "source": [
    "### Incorporating stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "planned-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_try = classi_identifier_full(train_text, classes_numeric, abstracts, to_csv = False, stopw = stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-magnitude",
   "metadata": {},
   "source": [
    "### Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "american-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_prob_complement(dictWordsFull, listdict):\n",
    "    '''Returns the probability of a word appearing given the class (format: dictionaries in list)'''\n",
    "    listdictprob = [{} for _ in range(len(unique_classes))]\n",
    "    for key in dictWordsFull:\n",
    "        for classi in range(0,len(unique_classes)):\n",
    "            listdictprob[classi][key] = 1/((dictWordsFull[key]-listdict[classi][key]+1)/(sum(dictWordsFull.values())-sum(listdict[classi].values())))\n",
    "                \n",
    "    return listdictprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "upper-ensemble",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def class_calculate_mult(words, listdictprob, dicY_prob):\n",
    "    '''Returns the most likely class based on Baysean Probability'''\n",
    "    classes = [[prob] for prob in dicY_prob.values()]\n",
    "    for word in words.split():\n",
    "        for classi in range(0,len(unique_classes)):\n",
    "            if word in listdictprob[classi]:\n",
    "                classes[classi].append(listdictprob[classi][word])\n",
    "    \n",
    "    minSum = []\n",
    "    for lst in classes:\n",
    "        minSum.append(np.prod(lst))\n",
    "\n",
    "    return minSum.index(min(minSum))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
